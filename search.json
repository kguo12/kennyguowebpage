[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "While I am still a student, here I share some papers and projects I’ve been working on, which mostly involve reinforcement learning and economics topics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoordinating for Clicks: Learning in Multi-Agent Information Asymmetric Cascading Bandits\n\n\n\n\n\n\nreinforcement learning\n\n\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\nKenny Guo, Lily Jiang, Lune Chan, Sophia Yi, William Chang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/homework 2/index.html",
    "href": "posts/homework 2/index.html",
    "title": "Webscraping with Scrapy: Movie Recommendations Based on Featured Actors",
    "section": "",
    "text": "Hello lovely reader! In this tutorial/assignment, we’ll be using the Scrapy Python library to webscrape The Movie Database (https://www.themoviedb.org/?language=en-US) in an attempt to get some show/movie recommendations depending on your favorite movie. Our idea is simple: if a movie has a bunch of the same actors as your favorite movie, it is likely a good recommendation.\n\nWebscraping Actors and Movies\nWe’ll get started by installing scrapy, then running in your terminal:\nscrapy startproject TMDB_scraper\nThis will create a directory equipped with a bunch of machinery for running “spiders” which will crawl around the webpages’ HTML code and parse it as we please. First, we’ll navigate to the /spiders folder, and create a new .py file called tmdb_spider.py. This is where we will code our spider.\nThe gameplan to make for our spider follows three steps: 1. Start on the movie’s webpage and navigate to its “Full Cast and Crew” page 2. Navigate to all the actors webpages on the cast page 3. Scrape the movies each actor was in and fill in the results in a CSV file\nWe’ll begin by importing scrapy and other important things like Spider and Request. To create our spider, we’ll define a TmdbSpider class that inherits from scrapy.Spider, so it is equipped with useful scrapy machinery.\nThen, we need to provide our spider a unique name. We’ll also define an initializer, so when we run our spider, we can provide in an argument for the subdirectory/favorite movie page we want our spider to start on. This will add the page url to start_urls, which will indicate for the spider to begin scraping this one first.\nfrom scrapy.spiders import Spider\nfrom scrapy.http import Request\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    # Unique name!\n    name = 'tmdb_spider'\n    # Provide your favorite movie's subdirectory\n    def __init__(self, subdir=\"\", *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n        \nAlright, so our spider is going to start with some movie page, but we also need to define a parse function which our spider will execute first on start_urls. Since the full cast and crew webpage can be found by simply appending /cast to the end of the url, we’ll do that.\nFrom there, we’ll send in a GET request using Request() to that new url, which will return another response object for us to parse. The function provided in the callback argument, parse_full_credits, instructs the spider how to parse this new response. We’ll define that next.\ndef parse(self, response):\n    '''\n    Assumes spider starts on a movie webpage, \n    navigates to the \"Full Cast & Crew\" page,\n    then runs parse_full_credits.\n    '''\n    cast_url = response.url + '/cast'\n    yield Request(cast_url, callback = self.parse_full_credits)\nOk, now our spider is on the full_cast and crew page. It needs to find all the links to the actors pages from the html in response.body. Some digging around on the html code for these webpages reveals that the people links can be found under ol tags with class=\"people_credits\". But these tags include lists for actors, crew members, production, etc., so to just get the list of actors, we’ll index out the first list.\nFrom there, we’ll parse the html further by finding all a tags embedded in p tags within the frist ol.people.credits tag (for actors), which contain the hyperlinks to the actors webpages. For each one, we’ll get the href attribute, and join it with the base url https://www.themoviedb.org/ which will give us the full url for the actor’s webpage.\nThen, following the gameplan, we’ll then send another GET request to each one of the actor’s pages, and pass in parse_actor_page to tell our spider how to parse the response we receive back. We’ll define that next.\n    def parse_full_credits(self, response):\n        '''\n        Assumes spider starts on a cast webpage, \n        navigates to each actors page, \n        then calls parse_actor_page.\n        '''\n        # Getting the urls to all actors pages\n        actors = response.css('ol.people.credits')[0].css('p a')\n        actor_urls = [response.urljoin(actor.attrib['href']) for actor in actors]\n        for url in actor_urls:\n            yield Request(url, callback = self.parse_actor_page)\nRight, so now our spider is now simultaneously crawling over all the actors webpages. First, we’ll get the actor’s full name, which can be found in the h2 tags with class=\"title\". We’ll CSS select just the text within the a tag, and assign the resulting string to actor_name.\nNext, we want to get the movies the actor has acted in. Unfortunately, the tables on the webpages can come in many different orders, sometimes starting with the “Acting” table, but other times, starting with “Crew” or “Production”. Fortunately, we can get the position of the “Acting” table with a little more CSS selecting. All of these tables will be in the div.credits_list tag, and each will have a header h3 tag, containing text for which table it holds. We’ll get these names one by one, and assign them to a list table_names. Then, within table_names, we’ll get just the index of the element \"Acting\", which will tell us the position the “Acting” table is in.\nFinally, using that index, we can select the right table.card.credits tag, containing the “Acting” table. Within these tables, there are a lot of dividers, but ultimately, all of the title names are stored within bdi tags. For each one, we’ll: - Access the text within them - Check if there’s a repeat (using a running set of title names) - If not, yield a dictionary with actor_name and movie_or_TV_name values for inputting into a CSV file\n    def parse_actor_page(self, response):\n        '''\n        Assumes spider starts on an actor's page, \n        then yields rows for a csv file containing \n        the actor's name and movies they have performed in.\n        '''\n        # Scraping actor name\n        actor_name = response.css('h2.title a::text').get()\n        \n        # Getting the right table number for \"Acting\"\n        table_names = [table.css('::text').get() for table in response.css('div.credits_list h3')]\n        table_num = [i for i in range(len(table_names)) if table_names[i] == \"Acting\"][0]\n        \n        # Scraping all titles, assigning to actor\n        titles = set()\n        for title in response.css('table.card.credits')[table_num].css('bdi'):\n            movie_or_TV_name = title.css('::text').get()\n            # Making sure no duplicate titles\n            if movie_or_TV_name not in titles:\n                titles.add(movie_or_TV_name)\n                yield {\n                    \"actor\": actor_name,\n                    \"movie_or_TV_name\": movie_or_TV_name\n                }\nTogether, our completed tmdb_spider.py file should look something like this:\nfrom scrapy.spiders import Spider\nfrom scrapy.http import Request\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    # Unique name!\n    name = 'tmdb_spider'\n    # Provide your favorite movie's subdirectory\n    def __init__(self, subdir=\"\", *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n        \n    def parse(self, response):\n        '''\n        Assumes spider starts on a movie webpage, navigates to the \"Full Cast & Crew\" page,\n        then runs parse_full_credits.\n        '''\n        cast_url = response.url + '/cast'\n        yield Request(cast_url, callback = self.parse_full_credits)\n        \n    def parse_full_credits(self, response):\n        '''\n        Assumes spider starts on a cast webpage, navigates to each actors page and calls parse_actor_page.\n        '''\n        # Getting the urls to all actors pages\n        actors = response.css('ol.people.credits')[0].css('p a')\n        actor_urls = [response.urljoin(actor.attrib['href']) for actor in actors]\n        for url in actor_urls:\n            yield Request(url, callback = self.parse_actor_page)\n        \n    def parse_actor_page(self, response):\n        '''\n        Assumes spider starts on an actor's page, and yields rows for a csv file containing the actor's name\n        and a movie they have performed in.\n        '''\n        # Scraping actor name\n        actor_name = response.css('h2.title a::text').get()\n        \n        # Getting the right table number for \"Acting\"\n        table_names = [table.css('::text').get() for table in response.css('div.credits_list h3')]\n        table_num = [i for i in range(len(table_names)) if table_names[i] == \"Acting\"][0]\n        \n        # Scraping all titles, assigning to actor\n        titles = set()\n        for title in response.css('table.card.credits')[table_num].css('bdi'):\n            movie_or_TV_name = title.css('::text').get()\n            # Making sure no duplicate titles\n            if movie_or_TV_name not in titles:\n                titles.add(movie_or_TV_name)\n                yield {\n                    \"actor\": actor_name,\n                    \"movie_or_TV_name\": movie_or_TV_name\n                }\nThen, to actually run this spider (or “crawl”), navigate to the TMBD_scraper directory in the command terminal and type in:\nscrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\nwhere subdir= can be filled in with your favorite movie (in this case, we’ve put in Harry Potter and the Philosopher’s Stone). This will yield a CSV file with an actor and movie_or_TV_name column, as we yielded in parse_actor_page. I’ve run this spider below and displayed the results in a pandas DataFrame.\n\nimport pandas as pd\nresults = pd.read_csv(\"results.csv\")\nresults\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nPaul Marc Davis\nArthur & Merlin: Knights of Camelot\n\n\n1\nPaul Marc Davis\nClass\n\n\n2\nPaul Marc Davis\nSon of God\n\n\n3\nPaul Marc Davis\nThe Bible\n\n\n4\nPaul Marc Davis\nThe Sky in Bloom\n\n\n...\n...\n...\n\n\n3076\nRichard Griffiths\nThe Sweeney\n\n\n3077\nRichard Griffiths\nNorma\n\n\n3078\nRichard Griffiths\nVillage Hall\n\n\n3079\nRichard Griffiths\nCrown Court\n\n\n3080\nRichard Griffiths\nTony Awards\n\n\n\n\n3081 rows × 2 columns\n\n\n\nGreat, we’ve completed the webscraping portion. Now, we let’s see what we can do with this data to make some recommendations.\n\n\nMaking Recommendations\nAlright, we want to group the movies/TV names and count the number of actors from our favorite movie that were featured in it. We can do this with the groupby function, then applying the count function to count the number of actors within each movie group.\nThen, we’ll sort by 'Number of Shared Actors', and return the top 10 movies with the most shared actors, which we assume to be pretty decent recommendations based on your favorite movie.\n\nrecs = results.groupby('movie_or_TV_name').apply('count').rename(columns={'actor':'Number of Shared Actors'})\nrecs = recs.sort_values(by='Number of Shared Actors', ascending=False)\nrecs.reset_index(inplace=True)\nrecs.head(10)\n\n\n\n\n\n\n\n\nmovie_or_TV_name\nNumber of Shared Actors\n\n\n\n\n0\nHarry Potter and the Philosopher's Stone\n62\n\n\n1\nHarry Potter and the Chamber of Secrets\n37\n\n\n2\nHarry Potter and the Prisoner of Azkaban\n26\n\n\n3\nHarry Potter and the Order of the Phoenix\n24\n\n\n4\nHarry Potter and the Deathly Hallows: Part 2\n23\n\n\n5\nHarry Potter and the Deathly Hallows: Part 1\n20\n\n\n6\nHarry Potter and the Half-Blood Prince\n19\n\n\n7\nHarry Potter and the Goblet of Fire\n19\n\n\n8\nDoctor Who\n13\n\n\n9\nHarry Potter 20th Anniversary: Return to Hogwarts\n11\n\n\n\n\n\n\n\nAlright, as expected, using “Harry Potter” as our favorite movie returned the rest of the movies in the Harry Potter franchise, as they would share the most actors. That’s not to say our recommendation assumption was wrong, if one hadn’t seen the rest of the movies before, these would arguably be the best recommendations. However, a more nuanced recommendation model could take into account many other different factors that simply your “favorite movie” to determine the best one to watch.\nNevertheless, let’s see if we can visualize these results in an appealing way to see the top recommendations in terms of shared actors. We can first make a simple bar chart ranking movies based on Shared Actors. It will display larger bars on top, so people can easily see movies that share a lot of the same actors.\n\nfrom matplotlib import pyplot as plt\n\ntop_recs = recs[9::-1]\n\nplt.figure(figsize=(8, 6))\nplt.barh(top_recs['movie_or_TV_name'], top_recs['Number of Shared Actors'])\nplt.title('Recommended Movies (based on number of shared actors with favorite)')\nplt.xlabel('Number of Shared Actors')\nplt.ylabel('Recommended Movies')\nplt.show()\n\n\n\n\n\n\n\n\nWe can also use the original actor-movie DataFrame to construct a crazy parallel categories plot using Plotly, showing how each actors association with each movie. Of course, this plot gets complicated fast, so we’ll limit it by first sorting the actor-movie pairs, ranking results by the number of shared actors in the movie, and then only plot the first 150 rows. This is more so a visualization to demonstrate the interconnectedness of actors and movies, rather than something for a user to utilize for recommendations :).\n\ncount = results.groupby('movie_or_TV_name').transform('count')\nresults['count'] = count\nresults = results.sort_values(by='count', ascending=False)\n\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nfig = px.parallel_categories(results[:150],\n                             dimensions = [\"actor\", \"movie_or_TV_name\"],\n                             height = 1000)\nfig.update_layout(margin={\"r\":100,\"t\":0,\"l\":100,\"b\":0})\nfig.show()\n\n\n\n\nAnd that’s it! As we’ve seen, using webscraping tools like scrapy can make gathering data implicitly stored on webpages and online databases nice and manageable, and we can use it for cool applications (such as recommendation systems like this assignment)."
  },
  {
    "objectID": "posts/homework 0/index.html",
    "href": "posts/homework 0/index.html",
    "title": "Palmer Penguins: Plotly Visualization Tutorial",
    "section": "",
    "text": "Who doesn’t love penguins? In this plotly visualization tutorial, we’ll be examining the “palmer_penguins” data set, graciously collected and published by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER (you can read more about the project and dataset here).\nIt contains data on 344 Anvers penguins of three species, Adelie, Chinstrap, and Gentoo, as well as various characteristics, such as their home island, the length and depth of their culmen, their flipper length, body mass, sex, and concentration of nitrogen and carbon in their bloodstream.\nLet’s first import in all our necessary libraries. In this tutorial, we’ll be constructing a simple visualization using plotly express. We’ll also import plotly.io and use the renderers framework so our figure can be displayed on this webpage. We’ll also import pandas and numpy to help with our initial data wrangling. Then, we’ll save the dataset into a dataframe called penguins.\nimport pandas as pd\nimport numpy as np\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\npenguins\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n339\nPAL0910\n120\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A2\nNo\n12/1/09\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n340\nPAL0910\n121\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN39A1\nYes\n11/22/09\n46.8\n14.3\n215.0\n4850.0\nFEMALE\n8.41151\n-26.13832\nNaN\n\n\n341\nPAL0910\n122\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN39A2\nYes\n11/22/09\n50.4\n15.7\n222.0\n5750.0\nMALE\n8.30166\n-26.04117\nNaN\n\n\n342\nPAL0910\n123\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN43A1\nYes\n11/22/09\n45.2\n14.8\n212.0\n5200.0\nFEMALE\n8.24246\n-26.11969\nNaN\n\n\n343\nPAL0910\n124\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN43A2\nYes\n11/22/09\n49.9\n16.1\n213.0\n5400.0\nMALE\n8.36390\n-26.15531\nNaN\n\n\n\n\n344 rows × 17 columns"
  },
  {
    "objectID": "posts/homework 0/index.html#data-wrangling",
    "href": "posts/homework 0/index.html#data-wrangling",
    "title": "Palmer Penguins: Plotly Visualization Tutorial",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nFor this simple visualization, our goal will be to somehow distinguish the three species of penguins; Adelie, Gentoo, and Chinstrap. For this, we’ll use a 2D graph, so we’ll only need two features. For this, we’ll only use the 'Flipper Length (mm)' and 'Culmen Length (mm)' columns.\nNote that some of the entries have NaN or missing values for flipper length and culmen length. We can treat these in a variety of ways, but for this example, we’ll simply remove them. As seen below, there were only 2 entries with missing values compared to 342 without, so this step is not too significant.\nFinally, for ease of reading, we’ll drop the “penguin” and the scientific name from the 'Species' column.\n\n# getting just these 3 columns\npenguins = penguins[['Species', 'Flipper Length (mm)', 'Culmen Length (mm)']]\n# dropping NaN values\npenguins = penguins.dropna()\n# getting just the species name\npenguins[\"Species\"] = penguins[\"Species\"].str.split().str.get(0)\n\npenguins\n\n\n\n\n\n\n\n\nSpecies\nFlipper Length (mm)\nCulmen Length (mm)\n\n\n\n\n0\nAdelie\n181.0\n39.1\n\n\n1\nAdelie\n186.0\n39.5\n\n\n2\nAdelie\n195.0\n40.3\n\n\n4\nAdelie\n193.0\n36.7\n\n\n5\nAdelie\n190.0\n39.3\n\n\n...\n...\n...\n...\n\n\n338\nGentoo\n214.0\n47.2\n\n\n340\nGentoo\n215.0\n46.8\n\n\n341\nGentoo\n222.0\n50.4\n\n\n342\nGentoo\n212.0\n45.2\n\n\n343\nGentoo\n213.0\n49.9\n\n\n\n\n342 rows × 3 columns"
  },
  {
    "objectID": "posts/homework 0/index.html#visualization-with-plotly",
    "href": "posts/homework 0/index.html#visualization-with-plotly",
    "title": "Palmer Penguins: Plotly Visualization Tutorial",
    "section": "Visualization with Plotly",
    "text": "Visualization with Plotly\nExcellent! Now to visualize, we’ll use plotly’s scatter plot. To do this, we’ll create a figure using px.scatter. It takes in a variety of parameters, including:\n\nOur penguins dataframe\nWhat columns to plot on the x and y axes\ncolor: colors the points based on their species\nWidth and height of the plot\n\nWe’ll also create some marginal histograms, which display the distribution of the data for one variable only. On the top we’ll see the distribution of flipper lengths across species, and on the right we’ll see the distribution of culmen lengths across species.\nFinally, we’ll use the fig.update_layout function to adjust some of our plot aesthetics, by adding a title, adjusting the margins, and editing the template style.\n\nfig = px.scatter(data_frame = penguins, \n                 x = \"Flipper Length (mm)\", \n                 y = \"Culmen Length (mm)\", \n                 color = \"Species\",\n                 width = 800,\n                 height = 500,\n                 marginal_y = \"histogram\",\n                 marginal_x = \"histogram\",\n                  )\n\n# Adjust the margins, add in a title, and set a plot template\nfig.update_layout(margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0}, \n                  title_text=\"Culmen Length vs. Flipper Length of the Three Anvers Penguin Species\",\n                  template=\"ggplot2\")\n\n# Show the plot\nfig.show()"
  },
  {
    "objectID": "posts/homework 0/index.html#discussion",
    "href": "posts/homework 0/index.html#discussion",
    "title": "Palmer Penguins: Plotly Visualization Tutorial",
    "section": "Discussion",
    "text": "Discussion\nFrom this plot, we can see some rough distinctions between the three species based on just these two features alone. Because of plotly’s nice interactive features, you can hover over any individual point as see the penguin’s species and it’s individual measurements.\nFrom the marginal histograms, we can even more clearly see that the flipper lengths allow us to distinguish Gentoo penguins apart fairly well, as they feature significantly higher flipper lengths on average, while the culmen lengths allow us to distinguish the Adelie penguins fairly well, as they feature significantly lower culmen lengths. Hover over any bucket and plotly will display the count of penguins within that range.\nThis insight could be useful for potentially training some models to predict the species of a penguin given its phenotype, track the evolution of these species across time, or assess any other general trends in the species’ populations."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kenny Guo",
    "section": "",
    "text": "Hi, welcome to my page! My name is Kenny and I’m a student at UCLA taking PIC16B: Python with Applications II (25W, S. Burnett). On this page, you’ll see some posts about some of my homework assignments and Pythoning expeditions in general. I also have some other projects and information in the tabs above. Thanks for visiting!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWebscraping with Scrapy: Movie Recommendations Based on Featured Actors\n\n\nPIC16B: Homework 2\n\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 8, 2025\n\n\nKenny Guo\n\n\n\n\n\n\n\n\n\n\n\n\nData Wrangling and Visualization: NOAA Climate Data\n\n\nPIC16B: Homework 1\n\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 2, 2025\n\n\nKenny Guo\n\n\n\n\n\n\n\n\n\n\n\n\nPalmer Penguins: Plotly Visualization Tutorial\n\n\nPIC16B: Homework 0\n\n\n\nhomework\n\n\n\n\n\n\n\n\n\nJan 22, 2025\n\n\nKenny Guo\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome!\n\n\n\n\n\n\ngeneral\n\n\nfun\n\n\n\n\n\n\n\n\n\nJan 21, 2025\n\n\nKenny Guo\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Kenny and I’m a student at UCLA. I’m studying math and economics."
  },
  {
    "objectID": "papers/multiagent cascading/index.html",
    "href": "papers/multiagent cascading/index.html",
    "title": "Coordinating for Clicks: Learning in Multi-Agent Information Asymmetric Cascading Bandits",
    "section": "",
    "text": "This is my applied math paper No. 1 (!), titled “Coordinating for Clicks: Learning in Multi-Agent Information Asymmetric Cascading Bandits”. I collaborated on this project alongside Lily, Lune, and Sophia, and our advisor was William (https://williamc.me/). You can view the manuscript (currently under review at JMLR) below.\nBandits are one of the most studied settings in reinforcement learning, with the standard multi-armed bandit featuring a “learning agent” who interacts with a feedback-giving “environment”. In stochastic bandits, the agent interacts over a series of rounds, called a horizon, \\(T\\). For each round, the agent has a variety of “actions” (or often dubbed “arms”) to choose from, each with it’s own reward distribution. Reinforcement learning is essentially learning how to optimally take in and learn from feedback in a new, unexplored environment, so it is the agent’s goal to learn the best/optimal arms to achieve the highest rewards.\nIn bandits, learners adopt a “policy”, often encoded in some algorithm. Researchers evaluate a policy based on its regret (\\(R_T\\)), which is the difference between the rewards received if the agent had played the optimal action over the entirety of the horizon and the actual rewards received following the algorithm. Ideally, a “good” algorithm should incur sublinear regret, that is, the regret grows on some order less than \\(O(T)\\). In the classical stochastic bandit, the Upper Confidence Bound (UCB) algorithm achieves the most optimal regret, on the order of \\(O(\\log T)\\). Essentially, the learner creates a confidence interval around the empirical reward estimate for each arm, and as arms are pulled and additional rewards are realized, these upper confidence bounds approach closer to the true mean reward for each arm. The agent then simply picks the action with the highest UCB index for each round, and with high probability, ends up finding the optimal arm. This is just a brief introduction—if you want to learn more, I recommend the book Bandit Algorithms, by Tor Lattimore and Csaba Szepesvari.\nThe bandit variant we study in this paper is called a cascading bandit. Instead of the learner choosing one action for each round, the agent can choose \\(K\\) different actions arranged in a list. This setting rifles with applications in things like recommendation bars or search engines, and for this reason, arms are sometimes referred to as “items” and the “action” taken for a round is now the full ranking/recommendation of \\(K\\) items.\nThe key feature of this environment is that the learner can receive partial feedback, in that even though they may recommend \\(K\\) items, they may receive feedback on less. This is because of the assumption that a “user” examines the items starting from the first item to the last item, and stops examining once they are “attracted” by an item. Thus, the remaining items are unobserved by the user, and the agent receives no feedback (a keen reader would note that in this setting, rewards are Bernoulli, so a “click” on an item results in a reward of \\(1\\), while an item observed and not clicked results in a reward of \\(0\\)). This is just a brief summary of the environment; if you’re curious, I’ve written more about it in the introduction section of the paper below, and you can look at the seminal paper on cascading bandits, linked here (https://arxiv.org/pdf/1502.02763).\nFor our main contribution, we study a multiplayer extension of cascading bandits. Instead of one agent interacting with the cascading environment, we consider \\(M\\) different learners, each with their own set of \\(L\\) items. The key distinction is that when each player recommends an item for a position in the ranking, it gives rise to a joint-item, which is a combination of each of the \\(M\\) players’ items. Each joint item has its own independent probability of attracting a user, meaning the agents now have to coordinate effectively to maximize rewards and minimize regret by selecting the \\(K\\) optimal joint-items. In addition to this, we study various forms of information asymmetry between the players, where players might not be able to observe the items played by others, or where they could receive different feedback from each other, or both. For an introduction to multiplayer bandits in the standard multi-armed bandit, I suggest reading this (https://arxiv.org/abs/2109.03818) paper, written by our advisor, William Chang.\nThis has been just a brief introduction. If you’re curious about it more, feel free to reach out at kennyguo@ucla.edu.\n\n    It appears you don't have a PDF plugin for this browser.\n    No biggie... you can click here to\n    download the PDF file."
  },
  {
    "objectID": "posts/homework 1/index.html",
    "href": "posts/homework 1/index.html",
    "title": "Data Wrangling and Visualization: NOAA Climate Data",
    "section": "",
    "text": "Hello! In this assignment, we’ll be wrangling some climate data gathered by stations in the National Oceanic and Atmospheric Association (NOAA) to produce some scatter plots for each stations average yearly increase in temperature. To do this, first we’ll import the relevant packages. We’ll need: - sqlite3: for database management - pandas: for working with DataFrames - plotly express: for geographic visualizations - scikit-learn: for linear regression\nimport pandas as pd\nimport sqlite3\nfrom plotly import express as px\nfrom sklearn.linear_model import LinearRegression\nimport plotly.io as pio\npio.renderers.default=\"iframe\""
  },
  {
    "objectID": "posts/homework 1/index.html#step-1.-create-a-database",
    "href": "posts/homework 1/index.html#step-1.-create-a-database",
    "title": "Data Wrangling and Visualization: NOAA Climate Data",
    "section": "Step 1. Create a Database",
    "text": "Step 1. Create a Database\nWe’ll begin by importing all our necessary files and throwing them all into our temps.db database. There is a lot of data, so using SQL will ultimately be more storage-conscious and we’ll be able to query from our tables more easily.\n\nif __name__ == \"__main__\":\n    # Read in CSV files\n    temps = pd.read_csv(\"temps_stacked.csv\")\n    temps.dropna()\n    countries = pd.read_csv(\"countries.csv\")\n    # editing whitespace and - to _\n    countries = countries.rename(columns={'FIPS 10-4': 'FIPS_10_4', 'ISO 3166': 'ISO_3166'})\n    stations = pd.read_csv(\"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/noaa-ghcn/station-metadata.csv\")\n    \n    # Open a connection to database\n    conn = sqlite3.connect(\"temps.db\")\n\n    # Adding the csv files\n    temps.to_sql(\"temperatures\", conn, if_exists = \"replace\", index = False)\n    countries.to_sql(\"countries\", conn, if_exists = \"replace\", index = False)\n    stations.to_sql(\"stations\", conn, if_exists = \"replace\", index = False)\n\n    # Closing connection\n    conn.close()"
  },
  {
    "objectID": "posts/homework 1/index.html#step-2.-create-a-query_climate_database-function",
    "href": "posts/homework 1/index.html#step-2.-create-a-query_climate_database-function",
    "title": "Data Wrangling and Visualization: NOAA Climate Data",
    "section": "Step 2. Create a query_climate_database Function",
    "text": "Step 2. Create a query_climate_database Function\nThis function will allow us to query our database (temps.db) to get information for stations within a certain timeframe for a certain country. In particular, it requests: - the country of interest - the month of interest - the start year and end year (inclusive) - the database to query\nand it returns: - a DataFrame containing the columns station name, station latitude, station longitude, country, year, month, and temperature (over the month).\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    '''\n    A function that queries a climate database, returning station information (name, lat, long)\n    and temperature data for a specific year frame and month, for a certain country. \n    Inputs: db_file (name of database), country (str), year_begin, year_end (int), month (int)\n    Returns: DataFrame\n    Example: query_climate_database(\"temps.db\", 'India', 1980, 2020, 1)\n    '''\n    conn = sqlite3.connect(db_file)\n    \n    # Getting the country code\n    cmd1 = \\\n    f\"\"\"\n    SELECT fips_10_4\n    FROM countries\n    WHERE name = '{country}'\n    \"\"\"\n    cell = pd.read_sql(cmd1, conn)\n    country_code = cell.iat[0,0]\n    \n    # SQL querying for relevant columns, subject to parameters\n    cmd2 = \\\n    f\"\"\"\n    SELECT S.name, S.latitude, S.longitude, '{country}' Country, T.year, T.month, T.temp\n    FROM temperatures T\n    LEFT JOIN stations S ON T.id = S.id\n    WHERE (SUBSTRING(T.id, 1, 2) = '{country_code}')\n    AND (T.year BETWEEN {year_begin} AND {year_end})\n    AND (T.month = {month})\n    \"\"\"\n    df = pd.read_sql(cmd2, conn)\n    \n    # Closing connection and returning dataframe\n    conn.close()\n    return df\n\nWe test our function here with \"India\", over the time period 1980-2020 for the month of January.\n\nquery_climate_database(\"temps.db\", 'India', 1980, 2020, 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns"
  },
  {
    "objectID": "posts/homework 1/index.html#step-3.-geographic-scatter-function-for-yearly-temperature-increases",
    "href": "posts/homework 1/index.html#step-3.-geographic-scatter-function-for-yearly-temperature-increases",
    "title": "Data Wrangling and Visualization: NOAA Climate Data",
    "section": "Step 3. Geographic Scatter Function for Yearly Temperature Increases",
    "text": "Step 3. Geographic Scatter Function for Yearly Temperature Increases\nIn this section, we seek to answer our overarching question: &gt; How does the average yearly change in temperature vary within a given country?\nTo do this, we define a function called temperature_coefficient_plot(). It takes in the same parameters as query_climate_database, and uses it to collect data for the selected country/timeframe. It then calculates for each station the average yearly change in temperature, by performing a linear regression using scikit-learn with years on the x-axis and temperature on the y-axis. The details of this process are defined in the helper function coef. Finally, it takes its calculated slopes and plots each station (along with its latitude, longitude) onto a plotly interactive map, with the stations colorcoded by average yearly change in temperature. It then returns this figure.\n\ndef coef(data_group):\n    '''\n    Helper function for temperature_coefficient_plot\n    Inputs: DataFrame containing temperatures over a year period\n    Outputs: Average yearly change in temperature (as calculated by a linear regression)\n    '''\n    X = data_group[[\"Year\"]] # Dataframe format\n    y = data_group['Temp'] # Series format\n    LR = LinearRegression()\n    LR.fit(X,y) # Calculates the coefficients\n    slope = LR.coef_[0] # Takes the slope (average yearly change in temp) from the model.\n    slope = round(slope, 4)\n    return slope\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    '''\n    A function that calculates the average yearly changes in temperature for stations in a certain country\n    over a certain time period over a certain month.\n    Inputs:\n    db_file (climate database), country (str), year_begin, year_end (int), month (int),\n    min_obs (int, the minimum number of years a station tracks data within the timeframe),\n    **kwargs (other keyword arguments for plotly map parameters)\n    Outputs:\n    A plotly figure containing points for stations, their lat/long, and average yearly change in temp.\n    '''\n    # Calling query_climate_database with parameters to get necessary data\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    \n    # Filtering out stations with less than min_obs\n    df['observations'] = df.groupby('NAME')['Year'].transform('count')\n    df = df[df['observations'] &gt; (min_obs-1)] \n    \n    # Grouping by station, calculating avg. yearly change in temp\n    coefs = df.groupby(['NAME', 'LATITUDE', 'LONGITUDE']).apply(coef)\n    coefs = coefs.reset_index()\n    coefs['Estimated Yearly Increase (°C)'] = coefs.iloc[:, 3]\n    # coefs is now a DataFrame with columns for the station, lat, long, and avg. change in temp\n    \n    # Plotting stations, coloring by avg. yearly change in temp\n    fig = px.scatter_mapbox(coefs, \n                        lat = \"LATITUDE\",\n                        lon = \"LONGITUDE\", \n                        hover_name = \"NAME\",\n                        color = \"Estimated Yearly Increase (°C)\",\n                        color_continuous_midpoint = 0,\n                        **kwargs) # now changing the style, this one is more low-contrast, keeps the coordinates out\n\n    fig.update_layout(title=f\"Estimates for Average Yearly Increase in Temperature (°C) for stations in {country}, years {year_begin}-{year_end}\",\n                      font=dict(size=10),\n                      margin={\"r\":0,\"t\":20,\"l\":0,\"b\":0})\n    \n    return fig\n    \n\nHere we test our function by plotting the temperature changes in India (1980-2020 in January) and China (1995-2022 in July).\n\ncolor_map = px.colors.diverging.RdGy_r \nfig = temperature_coefficient_plot(\"temps.db\", 'India', 1980, 2020, 1, \n                                    min_obs = 10,\n                                    zoom = 2,\n                                    mapbox_style=\"open-street-map\",\n                                    color_continuous_scale=color_map)\nfig.show()\n\n\n\n\n\nfig = temperature_coefficient_plot(\"temps.db\", 'China', 1995, 2022, 7, \n                                    min_obs = 10,\n                                    zoom = 2,\n                                    mapbox_style=\"open-street-map\",\n                                    color_continuous_scale=color_map)\nfig.show()\n\n\n\n\nIt seems we can draw some conclusions that over the past few decades, controlling for month/season, temperature has been rising at a fairly steady rate over the majority of the country for both India and China, as evidenced by the predominance of red/light red dots."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome!",
    "section": "",
    "text": "Hi, my name is Kenny!"
  }
]